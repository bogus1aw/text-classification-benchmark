{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M_herBERT_wiki.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "187x4Dk60yiA-7SGylQxMcxiqDV-9FMm_",
      "authorship_tag": "ABX9TyO7UJL+bEGeKkI1eLickkzU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bogus1aw/text-classification-benchmark/blob/main/M_herBERT_wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr7lMXUQe457"
      },
      "source": [
        "# HerBERT benchmark for wiki dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxEKhUQLd06w",
        "outputId": "af30e81a-210b-44ea-9512-797c24e5b9a6"
      },
      "source": [
        "# check available GPU\r\n",
        "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name, driver_version, memory.total [MiB]\n",
            "Tesla T4, 418.67, 15079 MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnkVSFqsi2PE",
        "outputId": "c1c6b0d7-7a9b-49ae-ca75-c39fc8a1a38f"
      },
      "source": [
        "!pip install datasets transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fH1WQT6fVzU",
        "outputId": "7fd85682-22a2-43f5-a057-19e972d778c3"
      },
      "source": [
        "from torch import cuda\r\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\r\n",
        "cuda.is_available()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NdSB-26gqPO"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "def load_corpora_to_dataframe(corpora):\r\n",
        "  data = open(corpora).read()\r\n",
        "  labels, texts = [], []\r\n",
        "  for i, line in enumerate(data.split(\"\\n\")):\r\n",
        "      content = line.split()\r\n",
        "      labels.append(content[0])\r\n",
        "      texts.append(\" \".join(content[1:]))\r\n",
        "  return texts, labels\r\n",
        "  # # create a dataframe using texts and labels\r\n",
        "  # trainDF = pd.DataFrame()\r\n",
        "  # trainDF['text'] = texts\r\n",
        "  # trainDF['label'] = labels\r\n",
        "  # return trainDF\r\n",
        "    \r\n",
        "raw_corpora = '/content/drive/MyDrive/master_datasets/wiki_preprocessed/wikiInOneFileDataset.txt'\r\n",
        "texts, labels = load_corpora_to_dataframe(raw_corpora)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDKLi0jLg-g8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "# create test dataset\r\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=.2)\r\n",
        "# create train and validation dataset\r\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxbXjdYbnt3e",
        "outputId": "b052e0c7-823f-438f-bfc9-c466e66232dd"
      },
      "source": [
        "print(len(train_texts), ' ', len(train_labels))\r\n",
        "print(len(val_texts), ' ', len(val_labels))\r\n",
        "print(len(test_texts), ' ', len(test_labels))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4406   4406\n",
            "1102   1102\n",
            "1377   1377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFk2_r10hu1c",
        "outputId": "ac2acdb3-7cf9-4e5a-cdb8-fd6548041200"
      },
      "source": [
        "import torch\r\n",
        "from transformers import HerbertTokenizer, RobertaForSequenceClassification, EvalPrediction\r\n",
        "\r\n",
        "tokenizer = HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\r\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"allegro/herbert-klej-cased-v1\", num_labels=34)\r\n",
        "\r\n",
        "# encoded_input = tokenizer.encode(\"Kto ma lepszą sztukę, ma lepszy rząd – to jasne.\", return_tensors='pt')\r\n",
        "# outputs = model(encoded_input)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allegro/herbert-klej-cased-v1 were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-klej-cased-v1 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxYaJhgojBQU"
      },
      "source": [
        "max_length = 200\r\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\r\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length)\r\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\r\n",
        "\r\n",
        "from sklearn import preprocessing\r\n",
        "encoder = preprocessing.LabelEncoder()\r\n",
        "train_labels = encoder.fit_transform(train_labels)\r\n",
        "val_labels = encoder.fit_transform(val_labels)\r\n",
        "test_labels = encoder.fit_transform(test_labels)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SLXi-HMjK2U"
      },
      "source": [
        "# build pyTorch dataset\r\n",
        "import torch\r\n",
        "\r\n",
        "class wikiDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, encodings, labels):\r\n",
        "        self.encodings = encodings\r\n",
        "        self.labels = labels\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\r\n",
        "        return item\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.labels)\r\n",
        "\r\n",
        "train_dataset = wikiDataset(train_encodings, train_labels)\r\n",
        "val_dataset = wikiDataset(val_encodings, val_labels)\r\n",
        "test_dataset = wikiDataset(test_encodings, test_labels)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "gBC0vCZyjpMm",
        "outputId": "fcb7c55f-42ae-4737-85fc-61dcf97508cc"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\r\n",
        "\r\n",
        "training_args = TrainingArguments(\r\n",
        "    output_dir='./results',          # output directory\r\n",
        "    evaluation_strategy = \"epoch\",\r\n",
        "    num_train_epochs=5,              # total number of training epochs\r\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\r\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\r\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\r\n",
        "    weight_decay=0.01,               # strength of weight decay\r\n",
        "    logging_dir='./logs',            # directory for storing logs\r\n",
        "    logging_steps=10,\r\n",
        "    load_best_model_at_end=True,\r\n",
        "    metric_for_best_model=\"accuracy\"\r\n",
        ")\r\n",
        "\r\n",
        "from datasets import load_metric\r\n",
        "import numpy as np\r\n",
        "metric = load_metric('accuracy')\r\n",
        "\r\n",
        "def compute_metrics(eval_pred):\r\n",
        "    predictions, labels = eval_pred\r\n",
        "    predictions = np.argmax(predictions, axis=1)\r\n",
        "    print(predictions[:10])\r\n",
        "    print(labels[:10])\r\n",
        "    return metric.compute(predictions=predictions, references=labels)\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\r\n",
        "    args=training_args,                  # training arguments, defined above\r\n",
        "    train_dataset=train_dataset,         # training dataset\r\n",
        "    eval_dataset=val_dataset,             # evaluation dataset\r\n",
        "    tokenizer=tokenizer,\r\n",
        "    compute_metrics=compute_metrics\r\n",
        ")\r\n",
        "trainer.train()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1380/1380 16:12, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.190700</td>\n",
              "      <td>0.318187</td>\n",
              "      <td>0.925590</td>\n",
              "      <td>13.580600</td>\n",
              "      <td>81.145000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.219600</td>\n",
              "      <td>0.358241</td>\n",
              "      <td>0.928312</td>\n",
              "      <td>13.752400</td>\n",
              "      <td>80.131000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.135200</td>\n",
              "      <td>0.365130</td>\n",
              "      <td>0.930127</td>\n",
              "      <td>13.780500</td>\n",
              "      <td>79.968000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.039800</td>\n",
              "      <td>0.352544</td>\n",
              "      <td>0.940109</td>\n",
              "      <td>13.781100</td>\n",
              "      <td>79.964000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.356182</td>\n",
              "      <td>0.940109</td>\n",
              "      <td>13.762400</td>\n",
              "      <td>80.073000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[20 26 32 19  0  1 17  5  0 15]\n",
            "[29 26 32 19  0  1 17  5  0 15]\n",
            "[20 26 32 19  0  1 17  5  0 15]\n",
            "[29 26 32 19  0  1 17  5  0 15]\n",
            "[29 26  6 19  0  1 17  5  0 15]\n",
            "[29 26 32 19  0  1 17  5  0 15]\n",
            "[20 26  6 19  0  1 17  5  0 15]\n",
            "[29 26 32 19  0  1 17  5  0 15]\n",
            "[20 26  6 19  0  1 17  5  0 15]\n",
            "[29 26 32 19  0  1 17  5  0 15]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1380, training_loss=0.1745831430390261, metrics={'train_runtime': 973.5816, 'train_samples_per_second': 1.417, 'total_flos': 3301812463368000, 'epoch': 5.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "-0M9V1LgZ3uw",
        "outputId": "720ab755-bf65-4c95-900c-e286933228a4"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18/18 00:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[20 26  6 19  0  1 17  5  0 15]\n",
            "[29 26 32 19  0  1 17  5  0 15]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 5.0,\n",
              " 'eval_accuracy': 0.9401088929219601,\n",
              " 'eval_loss': 0.35254397988319397,\n",
              " 'eval_runtime': 13.2114,\n",
              " 'eval_samples_per_second': 83.413}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "mh5Tz8Pbpg9k",
        "outputId": "3bc55474-cbc1-4cc1-e396-7bb0ad2a5fda"
      },
      "source": [
        "trainer.predict(test_dataset)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='40' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18/18 01:34]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[28 26  9  6 26 33  5  2 13 26]\n",
            "[28 26  9  6 26 33  5  2 13 26]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-0.5571416 ,  0.18439111,  0.52761453, ...,  0.9496374 ,\n",
              "        -0.6464733 , -1.6530906 ],\n",
              "       [-0.55233616,  0.5646693 , -0.24755786, ..., -0.14540587,\n",
              "        -0.60143787,  0.34126842],\n",
              "       [-0.46673205,  0.2631073 , -0.6643272 , ...,  0.03545144,\n",
              "        -0.82061845, -1.1058676 ],\n",
              "       ...,\n",
              "       [-0.6099048 ,  9.369973  ,  0.10133141, ..., -0.20009485,\n",
              "         0.08358958, -0.32795525],\n",
              "       [ 1.0136505 , -0.6784751 , -0.17293319, ..., -0.9429146 ,\n",
              "         0.1761758 , -0.723683  ],\n",
              "       [-0.31450817, -0.48556608, -0.4629359 , ...,  1.8547591 ,\n",
              "         1.5557394 , -0.12153225]], dtype=float32), label_ids=array([28, 26,  9, ...,  1, 24, 20]), metrics={'eval_loss': 0.34637635946273804, 'eval_accuracy': 0.9310094408133623, 'eval_runtime': 15.6207, 'eval_samples_per_second': 88.152})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ksmwAV0F0sr"
      },
      "source": [
        "# from torch.utils.data import DataLoader\r\n",
        "# from transformers import  AdamW\r\n",
        "\r\n",
        "# # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "# device = torch.device('cpu')\r\n",
        "\r\n",
        "# model.to(device)\r\n",
        "# model.train()\r\n",
        "\r\n",
        "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\r\n",
        "\r\n",
        "# optim = AdamW(model.parameters(), lr=5e-5)\r\n",
        "\r\n",
        "# for epoch in range(3):\r\n",
        "#     for batch in train_loader:\r\n",
        "#         optim.zero_grad()\r\n",
        "#         input_ids = batch['input_ids'].to(device)\r\n",
        "#         attention_mask = batch['attention_mask'].to(device)\r\n",
        "#         labels = batch['labels'].to(device)\r\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\r\n",
        "#         loss = outputs[0]\r\n",
        "#         loss.backward()\r\n",
        "#         optim.step()\r\n",
        "\r\n",
        "# model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}